In order to make the course as useful as possible to the most people, please fill out [this survey](https://docs.google.com/forms/d/e/1FAIpQLSeEFAbBObNEkY4VoCX4Vbj4D-6NwhCMXoyN3GnVhe6Jv4srtg/viewform) !

# Course description

Traditionally, the primary source of data in pragmatics has been researchers’ intuitions about utterance meanings. However, small numbers of introspective judgments about examples hand-selected by researchers who themselves provide these judgments, introduce bias into the  phenomena under investigation.  The recently emerging use of experimental methods for probing linguistically untrained  hearers’ interpretations has ameliorated the bias introduced by small numbers of judgments. It cannot, however, remove item bias: researchers  artificially construct the stimuli used in experiments. Fortunately, studying corpora of naturally occurring language can reduce item bias. Corpora provide naturally occurring utterances that can be used in tandem with platforms like Mechanical Turk to provide large-scale crowd-sourced interpretations of these utterances, thereby allowing for constructing large databases of different types of meanings (e.g., implicatures) in context. The course will introduce students to the use of corpora of naturally occurring language for research in semantics/pragmatics.

# Syllabus

## Day 1

1. Why use corpora (in Linguistics generally, in Pragmatics in particular)?
2. Types of questions one can and cannot answer using corpora
3. Relation to other kinds of data (introspection, controlled psycholinguistic experiments in the lab and on the web)
4. Overview of general corpus features:
	- size, genre, modality (spoken, written), accessibility
	- annotation: POS, syntax, reference, information status, prosody, social meta-information about speakers (e.g., gender, dialect, education level)
5. Intro to class project domain: projection behavior of (non-/semi-)factive verbs


## Day 2

TGrep2 tutorial -- saerch patterns based on regular expressions
- search patterns used in Degen 2015
- formulate search patterns to find interesting verbs and their contexts

## Day 3

TDTlite tutorial -- building a database
- walk through generating database used in Degen 2015
- useful types of information to extract: lexical/syntactic context, word frequencies, ngram frequencies, word lengths, phonology, prosody, social information
- extending/annotating the database with crowd-sourced pragmatic judgments

R data analysis/visualization tutorial?

## Day 4

Build a preliminary database of verbs

## Day 5

Discussion, wrapup, issues, further directions, buffer


# Readings/resources

- de Marneffe, Marie-Catherine and Christopher Potts (2014). Developing linguistic theories using annotated corpora. To appear in Nancy Ide and James Pustejovsky, eds., The Handbook of Linguistic Annotation. Berlin: Springer. [pdf](http://web.stanford.edu/~cgpotts/papers/demarneffe-potts-lingann.pdf)
- Degen, J. (2015). Investigating the distribution of some (but not all) implicatures using corpora and web-based methods. Semantics and Pragmatics, 8 (11), pp. 1-55. [pdf](http://semprag.org/article/view/sp.8.11)
- [TGrep2](https://tedlab.mit.edu/~dr/Tgrep2/) and the TGrep2 User Manual [pdf](https://tedlab.mit.edu/~dr/Tgrep2/tgrep2.pdf)
- [TDTlite](https://github.com/thegricean/TDTlite) and the TDTlite User Manual [\[pdf\]](https://github.com/thegricean/TDTlite/blob/master/docs/tdt_manual.pdf)